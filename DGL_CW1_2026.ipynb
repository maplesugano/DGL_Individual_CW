{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdVuWvBQM24U"
      },
      "source": [
        "# DGL 2026 Coursework 1\n",
        "\n",
        "- Explanations should be supported by evidence (e.g. plots, statistics, or controlled comparisons), not only intuition.\n",
        "- Plots or metrics must be accompanied by an explanation of what they reveal and why this explains the model behavior\n",
        "- This coursework is completed in a **single Jupyter notebook**.  \n",
        "- You may reuse helper functions across questions unless explicitly stated otherwise.  \n",
        "- Your final submission **must run correctly with “Restart Kernel & Run All”**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiREO3vIM22L"
      },
      "source": [
        "\n",
        "\n",
        "## Question 1 - Node Classification in Heterogeneous Spaces\n",
        "\n",
        "You are given a **bipartite** graph with two node types:\n",
        "- **Drugs**: chemical fingerprints (sparse vectors)\n",
        "- **Proteins**: sequence embeddings (dense vectors)\n",
        "\n",
        "The feature spaces are **disjoint** (different semantics, different dimensions). You must predict **drug labels**.\n",
        "\n",
        "### Rules\n",
        "- ✅ Allowed: Python, NumPy, PyTorch, matplotlib, scikit-learn, seaborn\n",
        "- ❌ Not allowed: DGL, PyG, or any prebuilt GNN/attention layers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /Users/kaedesugano/.pyenv/versions/3.12.3/lib/python3.12/site-packages (26.0.1)\n",
            "Requirement already satisfied: numpy in /Users/kaedesugano/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f1loOcvvf9vs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Standard Library\n",
        "# =========================\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "# =========================\n",
        "# Numerical & Scientific\n",
        "# =========================\n",
        "import numpy as np\n",
        "\n",
        "# =========================\n",
        "# PyTorch\n",
        "# =========================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# =========================\n",
        "# Graph & Geometry\n",
        "# =========================\n",
        "import networkx as nx\n",
        "from graph_ricci_curvature import GraphRicciCurvature\n",
        "\n",
        "# =========================\n",
        "# Visualisation\n",
        "# =========================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# =========================\n",
        "# Evaluation & Analysis\n",
        "# =========================\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# =========================\n",
        "# Reproducibility\n",
        "# =========================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================\n",
        "# Device\n",
        "# =========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu2Wamxti33E"
      },
      "source": [
        "### 1.1 Dataset (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnZKkZpAgCcT"
      },
      "source": [
        "#### Load dataset\n",
        "The dataset is stored as `.npz` and contains:\n",
        "- `Xd`: drug features `[n_drugs, d_drug]`\n",
        "- `Xp`: protein features `[n_proteins, d_protein]`\n",
        "- `y`: drug labels `[n_drugs]`\n",
        "- `edges_dp`: edges `[E,2]` of `(drug_id, protein_id)`\n",
        "- `train_mask`, `val_mask`, `test_mask`: boolean masks over drugs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1gzeAVAgI0g"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"data/synth_drug_protein_np.npz\"  # update path if needed\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found: {DATA_PATH}\"\n",
        "\n",
        "data = np.load(DATA_PATH, allow_pickle=True)\n",
        "\n",
        "Xd = torch.tensor(data[\"Xd\"], dtype=torch.float32)\n",
        "Xp = torch.tensor(data[\"Xp\"], dtype=torch.float32)\n",
        "y  = torch.tensor(data[\"y\"], dtype=torch.long)\n",
        "edges_dp = torch.tensor(data[\"edges_dp\"], dtype=torch.long)  # [E,2]\n",
        "\n",
        "train_mask = torch.tensor(data[\"train_mask\"], dtype=torch.bool)\n",
        "val_mask   = torch.tensor(data[\"val_mask\"], dtype=torch.bool)\n",
        "test_mask  = torch.tensor(data[\"test_mask\"], dtype=torch.bool)\n",
        "\n",
        "n_drugs, d_drug = Xd.shape\n",
        "n_proteins, d_protein = Xp.shape\n",
        "E = edges_dp.shape[0]\n",
        "n_classes = int(y.max().item()) + 1\n",
        "\n",
        "print(\"Xd:\", Xd.shape)\n",
        "print(\"Xp:\", Xp.shape)\n",
        "print(\"edges_dp:\", edges_dp.shape)\n",
        "print(\"y:\", y.shape, \"classes:\", n_classes)\n",
        "print(\"splits:\", train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LUf7QUM20X"
      },
      "source": [
        "#### 1.1.a Exploratory Data Analysis (2 pts)\n",
        "**Task:** Calculate the following statistics to understand the \"Semantic Gap.\"\n",
        "1. The **sparsity** (percentage of zeros) of the Drug features.\n",
        "2. The **mean and standard deviation** of the Protein features.\n",
        "3. The **average degree** (number of protein neighbors) per drug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt4uldJsgjka"
      },
      "outputs": [],
      "source": [
        "def analyze_feature_gap(Xd, Xp, edges_dp):\n",
        "    # TODO: Calculate and print the requested statistics\n",
        "    pass\n",
        "\n",
        "analyze_feature_gap(Xd, Xp, edges_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O92M2VhQgqWI"
      },
      "source": [
        "#### 1.1.b Feature Density Analysis (2 pts)\n",
        "A critical part of working with heterogeneous graphs is understanding the numerical distribution of different node types. In this bipartite graph, Drugs and Proteins come from entirely different semantic spaces (binary fingerprints vs. dense embeddings).\n",
        "\n",
        "Task: Implement the `plot_feature_density` function to visualize the distribution of average feature values for all Drugs versus all Proteins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPF8kwEPjRTO"
      },
      "outputs": [],
      "source": [
        "def plot_feature_density(Xd, Xp):\n",
        "    \"\"\"\n",
        "    Plots the density of average feature values for all Drugs vs all Proteins.\n",
        "\n",
        "    Args:\n",
        "        Xd: Drug features [n_drugs, d_drug]\n",
        "        Xp: Protein features [n_proteins, d_protein]\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # 1. Calculate average feature values per node (mean across the feature dimension)\n",
        "    # TODO: Calculate avg_d and avg_p\n",
        "\n",
        "    # 2. Use sns.kdeplot to visualize the distributions\n",
        "    # Hint: Use fill=True and distinct colors to show the two \"islands\" of data.\n",
        "\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "    plt.title(\"Distribution of *average* feature values: Drugs vs Proteins\")\n",
        "    plt.xlabel(\"Average feature value\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend([\"Drugs\", \"Proteins\"])\n",
        "    plt.show()\n",
        "\n",
        "# plot_feature_density(Xd, Xp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVJ32W1hjljL"
      },
      "source": [
        "#### 1.1.c t-SNE Gap Visualization (2 pts)\n",
        "**Task:** Use t-SNE to project both drugs and proteins into a 2D space.\n",
        "\n",
        "**Hint:** Drug and protein features have different dimensions and cannot be concatenated directly.\n",
        "You must first project them into the same dimension using a fixed random projection\n",
        "(e.g., 128 dimensions) before running t-SNE.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T94I357gI64i"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "def fixed_random_projection(X, d_out=128, seed=0):\n",
        "    \"\"\"\n",
        "    Projects features to a shared dimension using a fixed random matrix.\n",
        "    This projection is non-learnable and used only for visualization.\n",
        "    \"\"\"\n",
        "    g = torch.Generator(device=X.device)\n",
        "    g.manual_seed(seed)\n",
        "    R = torch.randn(X.size(1), d_out, generator=g, device=X.device)\n",
        "    R = R / (X.size(1) ** 0.5)\n",
        "    return X @ R\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KH90HN6jqRH"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "def plot_feature_spaces(Xd, Xp, n_each=400):\n",
        "    # TODO:\n",
        "    # 1. Subsample nodes for fast execution\n",
        "    # 2. Project Xd and Xp to a shared 128-dim space using a fixed random matrix\n",
        "    # 3. Run TSNE (from sklearn, check imports) and plot with different colors for drugs/proteins\n",
        "    pass\n",
        "\n",
        "plot_feature_spaces(Xd, Xp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rff15s7GyP3G"
      },
      "source": [
        "#### 1.1.d Graph Visualization (2 pts)\n",
        "You must visualize **a small part** of the bipartite graph:\n",
        "- sample `n_drug_sample` drugs\n",
        "- include their connected proteins\n",
        "- draw the bipartite subgraph\n",
        "\n",
        "Your plot should make the bipartite structure obvious (two vertical columns: drugs on left, proteins on right)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU4niBo5yPo-"
      },
      "outputs": [],
      "source": [
        "def sample_bipartite_subgraph(edges_dp: torch.Tensor,\n",
        "                              n_drugs: int,\n",
        "                              n_drug_sample: int = 30,\n",
        "                              seed: int = 42):\n",
        "    \"\"\"\n",
        "    Sample a subgraph:\n",
        "      - choose n_drug_sample drugs uniformly at random\n",
        "      - include all proteins they connect to\n",
        "      - return lists of (drug_ids, protein_ids, edges_sub)\n",
        "\n",
        "    edges_dp: [E,2] (drug_id, protein_id), does NOT need to be sorted for this function\n",
        "    \"\"\"\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO:\n",
        "    # 1) sample drug ids\n",
        "    # 2) filter edges where drug in sampled set\n",
        "    # 3) collect unique protein ids appearing in those edges\n",
        "    # 4) reindex node ids locally for plotting convenience (optional but recommended)\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "\n",
        "def plot_bipartite_subgraph(drug_ids_global,\n",
        "                            prot_ids_global,\n",
        "                            edges_sub_global,\n",
        "                            title: str = \"Bipartite subgraph (Drugs ↔ Proteins)\",\n",
        "                            max_edges: int = 300):\n",
        "    \"\"\"\n",
        "    Plot bipartite graph with a simple two-column layout:\n",
        "      - drugs at x=0\n",
        "      - proteins at x=1\n",
        "    and edges drawn as line segments.\n",
        "\n",
        "    If there are too many edges, you may subsample edges to max_edges for readability.\n",
        "    \"\"\"\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO:\n",
        "    # 1) assign y-positions to drugs and proteins (e.g., equally spaced)\n",
        "    # 2) draw nodes as scatter points (different marker/color for drugs vs proteins)\n",
        "    # 3) draw edges as faint lines\n",
        "    # 4) add legend/title\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "\n",
        "# Example usage (uncomment after implementation)\n",
        "# drug_ids_g, prot_ids_g, edges_sub_g = sample_bipartite_subgraph(edges_dp, n_drugs=n_drugs, n_drug_sample=30)\n",
        "# plot_bipartite_subgraph(drug_ids_g, prot_ids_g, edges_sub_g)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McVhUT9Yjzmr"
      },
      "source": [
        "#### 1.1.e Theoretical analysis (2 pts)\n",
        "Consider a naive approach: project drugs and proteins into the same hidden dimension with linear layers, then mean/sum aggregate.\n",
        "\n",
        "Explain why this is suboptimal in this bipartite setting, referring to:\n",
        "- **Over-smoothing / signal dilution**\n",
        "- **Manifold alignment**\n",
        "\n",
        "Write your answer below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yGF0AzfkHe_"
      },
      "source": [
        "### 1.2 Cross-Modal Attention (25 pts)\n",
        "\n",
        "In this section, you will implement a custom Cross-Modal Attention (CMA) layer to update drug node representations using information from their protein neighbors. Because the graph is bipartite and the feature spaces are disjoint, we treat the **Drug** as the Query and the **Proteins** as Keys and Values.\n",
        "\n",
        "#### Mathematical Formulation\n",
        "\n",
        "The updated representation for a drug node $u$ is calculated using a residual connection:\n",
        "\n",
        "$$h_u^{(l+1)} = \\sigma\\left(\\sum_{v \\in N(u)} \\alpha_{uv} W_{val} h_v^{(l)} + h_u^{(l)}\\right)$$\n",
        "\n",
        "The attention coefficient $\\alpha_{uv}$ weights the importance of protein $v$ to drug $u$. It is computed via a neighborhood-specific softmax:\n",
        "\n",
        "$$\\alpha_{uv} = \\text{softmax}_{v \\in N(u)} \\left( \\text{LeakyReLU} \\left( a^T [W_q h_u^{(l)} \\Vert W_k h_v^{(l)}] \\right) \\right)$$\n",
        "\n",
        "\n",
        "\n",
        "#### Symbol Definitions\n",
        "\n",
        "* $h_u^{(l)}$: The feature vector of the **central drug** node $u$ at layer $l$.\n",
        "* $h_v^{(l)}$: The feature vector of the **neighboring protein** node $v$ at layer $l$.\n",
        "* $N(u)$: The set of protein neighbors connected to drug $u$.\n",
        "* $W_q$: Learnable weight matrix for the **Query** transformation (applied to Drugs).\n",
        "* $W_k$: Learnable weight matrix for the **Key** transformation (applied to Proteins).\n",
        "* $W_{val}$: Learnable weight matrix for the **Value** transformation (applied to Proteins).\n",
        "* $a^T$: A learnable attention vector that reduces the concatenated Query-Key vector to a single scalar score.\n",
        "* $\\Vert$: The concatenation operation.\n",
        "* $\\sigma$: A non-linear activation function (e.g., ReLU).\n",
        "\n",
        "#### Requirements\n",
        "\n",
        "* **No Pre-built Layers**: You must implement the logic using basic PyTorch operations. You may use `nn.Linear` or `nn.Parameter` to define the weights.\n",
        "* **Query/Key Separation**: Ensure $W_q$ is only used for drug nodes and $W_k, W_{val}$ are only used for protein nodes.\n",
        "* **Neighborhood Softmax**: The softmax must be normalized strictly over the neighbors of each individual drug. Use the `ptr_drug` array to identify these segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE9DbqYVlE0c"
      },
      "source": [
        "#### 1.2.a Segment softmax helper (5 pts)\n",
        "In a standard GNN, we often have a long 1D tensor of scores (one for every edge in the graph). However, attention must be a probability distribution **per node neighborhood**. To achieve this, we use a \"segment softmax.\"\n",
        "\n",
        "#### How it works\n",
        "Since `edges_dp` is sorted by the drug (query) node index, all proteins connected to Drug 0 appear first, followed by all proteins for Drug 1, and so on. The `ptr_drug` array acts as a map to these segments:\n",
        "- The neighbors of drug $u$ are found in the slice `scores[ptr[u] : ptr[u+1]]`.\n",
        "- You must apply the softmax operation independently to each of these slices.\n",
        "\n",
        "#### Numerical Stability\n",
        "When implementing softmax, it is a best practice to use the \"Log-Sum-Exp\" trick or subtract the maximum value from the scores before exponentiating:\n",
        "$$\\text{softmax}(x_i) = \\frac{\\exp(x_i - \\max(x))}{\\sum \\exp(x_j - \\max(x))}$$\n",
        "This prevents `torch.exp()` from overflowing if the attention scores are large.\n",
        "\n",
        "\n",
        "\n",
        "#### Implementation Task\n",
        "Implement the function below. While high-performance GNNs use specialized kernels, for this coursework, a Python loop over the number of drugs is acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IncwISIZlIJm"
      },
      "outputs": [],
      "source": [
        "def segment_softmax(scores: torch.Tensor, ptr: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    Computes a numerically stable softmax over segments of an edge tensor.\n",
        "\n",
        "    Each segment corresponds to the neighborhood of a single drug node. This\n",
        "    ensures that the attention weights for each drug's protein neighbors\n",
        "    sum to 1.0.\n",
        "\n",
        "    Args:\n",
        "        scores: Tensor of shape [E] (one scalar per edge)\n",
        "        ptr: Tensor of shape [num_nodes + 1] defining neighborhood segments\n",
        "    Returns:\n",
        "        Tensor of shape [E], where softmax is applied independently per segment.\n",
        "\n",
        "    Notes:\n",
        "        To ensure numerical stability and avoid overflow, you should\n",
        "        subtract the maximum score within each segment before exponentiating:\n",
        "        softmax(x_i) = exp(x_i - max(x)) / sum(exp(x_j - max(x)))\n",
        "    \"\"\"\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO: Implement the stable per-segment softmax using a loop over drugs.\n",
        "    # 1. Identify segment bounds using ptr.\n",
        "    # 2. skip empty segments (if any drug has no neighbors)\n",
        "    # 3. Compute max per segment for stability.\n",
        "    # 4. Exponentiate and normalize.\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xlCqlZlLRQ"
      },
      "source": [
        "#### 1.2.b Cross-Modal Attention Layer (5 pts)\n",
        "\n",
        "Now you will implement the `CrossModalAttention` module. This layer is the core of the architecture, allowing drug nodes to \"listen\" to their protein neighbors and update their internal state based on biological relevance.\n",
        "\n",
        "#### Functional Requirements:\n",
        "1.  **Shared Latent Space**: Since drugs ($d_{drug}$) and proteins ($d_{protein}$) have different input dimensions, you must project them into a shared hidden dimension $d_{hidden}$ using learnable weights.\n",
        "2.  **Directional Processing**: In this bipartite setup, the drugs act as **Queries** ($Q$), while the proteins act as **Keys** ($K$) and **Values** ($V$).\n",
        "3.  **The Attention Mechanism**:\n",
        "    * For every edge $(u, v)$, compute a scalar score based on the concatenation of the drug query $q_u$ and protein key $k_v$.\n",
        "    * Apply a `LeakyReLU` activation to allow the model to learn non-linear relationships.\n",
        "    * Use your `segment_softmax` to normalize these scores into weights $\\alpha_{uv}$ that sum to 1 for each drug's neighborhood.\n",
        "4.  **Message Aggregation**: Multiply the protein values $v_v$ by the attention weights $\\alpha_{uv}$ and sum them up for each drug.\n",
        "5.  **Residual Connection**: Add the aggregated neighbourhood information back to the projected drug representation ($d_{hidden}$) to preserve the drug's own feature identity.\n",
        "\n",
        "\n",
        "\n",
        "#### Implementation Task\n",
        "Implement the `forward` pass using only basic PyTorch operations. Avoid using `for` loops where possible for the projection and score calculation; however, a loop is acceptable for the final aggregation across segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKLdvWNhlQvF"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Your code starts here\n",
        "# =========================\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\"Return updated drug reps.\"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "    \n",
        "# =========================\n",
        "# Your code ends here\n",
        "# =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TSTPHqplWvT"
      },
      "source": [
        "#### 1.2.c Discussion (10 pts)\n",
        "\n",
        "**Since the task is drug classification but the graph is bipartite, why is a 1-layer model structurally incapable of exploiting drug–drug information for this task? Explain mathematically.**\n",
        "\n",
        "**[Answer]**\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-W8GZ0bncC2"
      },
      "source": [
        "#### 1.2.d Heterogeneous CMA Classifier (5 pts)\n",
        "We recommend a **2-step** update to allow information flow via drug→protein→drug (2-hop paths).\n",
        "\n",
        "You will implement:\n",
        "- drug classifcation model using CMA layers\n",
        "- protein update from drugs\n",
        "- drug update from proteins\n",
        "\n",
        "See the docstrings in the class below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)  \n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Helper functions for edge re-indexing and CSR-style pointer creation\n",
        "# Provided utility code (you do not need to modify).\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def sort_edges_by_src(edges: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Sort edges by their source node index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    edges : torch.Tensor, shape (E, 2)\n",
        "        Edge list where each row is (src, dst).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor, shape (E, 2)\n",
        "        Edge list sorted in ascending order of src.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Sorting edges by source node is required before constructing\n",
        "    CSR-style pointer arrays (ptr), which assume contiguous blocks\n",
        "    of edges per source node.\n",
        "    \"\"\"\n",
        "    src = edges[:, 0]                 # Extract source node indices\n",
        "    order = torch.argsort(src)        # Indices that sort edges by src\n",
        "    return edges[order]               # Reorder edges accordingly\n",
        "\n",
        "\n",
        "def make_ptr_from_sorted_src(sorted_src: torch.Tensor, n_src: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Construct a CSR-style pointer array from sorted source indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sorted_src : torch.Tensor, shape (E,)\n",
        "        Source node indices sorted in ascending order.\n",
        "    n_src : int\n",
        "        Total number of source nodes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor, shape (n_src + 1,)\n",
        "        Pointer array where ptr[i] gives the start index of edges\n",
        "        originating from source node i in the sorted edge list.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This follows the Compressed Sparse Row (CSR) convention:\n",
        "    - Edges for node i are located in the range [ptr[i], ptr[i+1]).\n",
        "    - Nodes with no outgoing edges will have ptr[i] == ptr[i+1].\n",
        "    \"\"\"\n",
        "    counts = torch.bincount(sorted_src, minlength=n_src)  # Number of edges per source\n",
        "    ptr = torch.zeros(n_src + 1, dtype=torch.long)        # Allocate pointer array\n",
        "    ptr[1:] = torch.cumsum(counts, dim=0)                 # Cumulative sum of counts\n",
        "    return ptr\n",
        "\n",
        "\n",
        "def make_reverse_edges(edges_dp: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reverse the direction of edges.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    edges_dp : torch.Tensor, shape (E, 2)\n",
        "        Edge list in (drug_id, protein_id) format.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor, shape (E, 2)\n",
        "        Edge list in (protein_id, drug_id) format.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This is useful when switching the perspective of the graph,\n",
        "    e.g. from drug-centric neighborhoods to protein-centric ones.\n",
        "    \"\"\"\n",
        "    return torch.stack([edges_dp[:, 1], edges_dp[:, 0]], dim=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Build protein-centric edge representation\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "edges_pd = sort_edges_by_src(make_reverse_edges(edges_dp))\n",
        "# edges_pd now has shape (E, 2) with (protein_id, drug_id),\n",
        "# sorted by protein_id\n",
        "\n",
        "prot_src = edges_pd[:, 0]   # Protein node indices (sources)\n",
        "drug_dst = edges_pd[:, 1]   # Drug node indices (destinations)\n",
        "\n",
        "ptr_prot = make_ptr_from_sorted_src(prot_src, n_proteins)\n",
        "# ptr_prot defines edge ranges for each protein node in edges_pd\n",
        "\n",
        "print(\"edges_pd:\", edges_pd.shape)\n",
        "print(\"ptr_prot:\", ptr_prot.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIoT05nanwTu"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Your code starts here\n",
        "# =========================\n",
        "class HeteroCMAClassifier(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "# =========================\n",
        "# Your code ends here\n",
        "# =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M3lALodn4rg"
      },
      "source": [
        "### 1.3 Alignment Loss (5 pts)\n",
        "\n",
        "You must implement:\n",
        "$$L_{total} = L_{CE} + \\lambda L_{align}$$\n",
        "\n",
        "Where `L_align` pulls **connected** drug-protein pairs closer in a **shared latent space**.\n",
        "\n",
        "Hint: Consider distance measures between two embedding vectors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 1.3.a Implementation (2 pts)\n",
        "Implement `alignment_loss(...)` for connected edges only.\n",
        "Define `z_d` and `z_p` as the **d_hidden-dimensional embeddings** produced by your model (after projection into the shared latent space and before the final classifier). Alignment loss should only be computed for connected drug-protein pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeGkn9A-n-8l"
      },
      "outputs": [],
      "source": [
        "def alignment_loss(z_d: torch.Tensor, z_p: torch.Tensor, edges_dp: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    z_d: drug embeddings in shared alignment space [n_drugs, d_align]\n",
        "    z_p: protein embeddings in shared alignment space [n_proteins, d_align]\n",
        "    edges_dp: [E,2]\n",
        "    Return scalar alignment loss.\n",
        "    \"\"\"\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO: implement an alignment loss over connected pairs only\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpJYEFfpoF7M"
      },
      "source": [
        "#### Training utilities\n",
        "Implement:\n",
        "- a macro-F1 function\n",
        "- training loop that compares λ=0 vs λ>0 (only change λ, do not change other hyperparameters, number of layers etc.)\n",
        "- testing loop\n",
        "\n",
        "You must plot F1 curves and discuss the trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5bjHNHyoFfi"
      },
      "outputs": [],
      "source": [
        "def macro_f1(pred: torch.Tensor, target: torch.Tensor, n_classes: int) -> float:\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO: implement macro-F1\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "\n",
        "def train_model(*args, **kwargs):\n",
        "    \"\"\"Return a dict history with train/val F1 and losses.\"\"\"\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    # TODO:\n",
        "    # 1) move tensors to device\n",
        "    # 2) define optimizer\n",
        "    # 3) loop epochs:\n",
        "    #    - forward\n",
        "    #    - compute CE loss on train drugs\n",
        "    #    - compute alignment loss if lam>0\n",
        "    #    - backprop + step\n",
        "    #    - evaluate macro-F1 on train and val\n",
        "    # 4) return history\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "def test_model(*args, **kwargs):\n",
        "    # =========================\n",
        "    # Your code starts here\n",
        "    # =========================\n",
        "    raise NotImplementedError\n",
        "    # =========================\n",
        "    # Your code ends here\n",
        "    # =========================\n",
        "\n",
        "\n",
        "def plot_f1_curves(h0: dict, h1: dict, label0: str = \"lambda=0\", label1: str = \"lambda>0\"):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.plot(h0[\"val_f1\"], label=f\"val F1 ({label0})\")\n",
        "    plt.plot(h1[\"val_f1\"], label=f\"val F1 ({label1})\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Macro F1\")\n",
        "    plt.title(\"Validation F1 vs Epoch\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run your code here and plot F1 curves for different lambda values\n",
        "# report test F1 for both settings\n",
        "\n",
        "hyperparams = {...}\n",
        "\n",
        "model = HeteroCMAClassifier(...)\n",
        "history_lambda_0 = train_model(model, hyperparams, lam=0.0)\n",
        "\n",
        "model = HeteroCMAClassifier(...)\n",
        "history_lambda_1 = train_model(model, hyperparams, lam=0.1) # or any lam > 0\n",
        "\n",
        "plot_f1_curves(history_lambda_0, history_lambda_1, label0=\"lambda=0\", label1=\"lambda>0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db1OuJEZoOME"
      },
      "source": [
        "#### 1.3.b Analysis (3 pts)\n",
        "Compare λ=0 vs λ>0:\n",
        "- Did alignment help convergence speed? Why?\n",
        "- Did it improve peak validation F1?\n",
        "- Did it over-constrain the space?\n",
        "\n",
        "\n",
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3nvf1iZ-LGT"
      },
      "source": [
        "## Question 2 - Investigating Topology in Node-Based Classification Using GNNs (30 pts)\n",
        "\n",
        "In this section, we will explore the impact of graph topology on node-based classification using GNNs. The experiments will focus on analyzing different topological measures, visualizing their distributions, and evaluating GCN performance on 2 graphs with different topologies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtLujjck-Qen"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "\n",
        "# Data Paths\n",
        "G1_TRAIN_PATH = os.path.join(\"data\", \"q2_G1_train.json\")\n",
        "G1_EVAL_PATH  = os.path.join(\"data\", \"q2_G1_eval.json\")\n",
        "G2_TRAIN_PATH = os.path.join(\"data\", \"q2_G2_train.json\")\n",
        "G2_EVAL_PATH  = os.path.join(\"data\", \"q2_G2_eval.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzOfmVFJ-WuY"
      },
      "outputs": [],
      "source": [
        "def create_Adj_matrix(N, edge_index):\n",
        "    \"\"\"Creates the adjacency matrix from an edge index list.\"\"\"\n",
        "    A = torch.zeros((N, N), dtype=torch.float)\n",
        "    for idx, jdx in edge_index:\n",
        "        A[idx, jdx] = 1\n",
        "        A[jdx, idx] = 1\n",
        "    return A\n",
        "\n",
        "def read_json_data(file_path, has_label=True):\n",
        "    \"\"\"\n",
        "    Loads data from a JSON file.\n",
        "    Returns: list of (X, A, y) tuples.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"{file_path} not found.\")\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Normalize to list format even if single graph\n",
        "    if not isinstance(data, list):\n",
        "        data = [data]\n",
        "\n",
        "    graph_data = []\n",
        "    for item in data:\n",
        "        X = torch.tensor(item['features'], dtype=torch.float).to(device)\n",
        "        N = len(X)\n",
        "        A = create_Adj_matrix(N, item['edge_index']).to(device)\n",
        "        y = torch.tensor(item['label'], dtype=torch.long).to(device) if has_label else None\n",
        "        graph_data.append((X, A, y))\n",
        "\n",
        "    return graph_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2.1 - Analyzing the Graphs (10 pts)\n",
        "\n",
        "#### 2.1.a Topological and Geometric Measures (3 pts)\n",
        "\n",
        "* Examine two topological measures (**Node Degree** and **Betweenness Centrality**) and one geometric measure (**Ollivier-Ricci Curvature**).\n",
        "* Include a definition for each measure.\n",
        "* Explain **mathematically** and **intuitively** what each measure quantifies and how it contributes to understanding the structure of a graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LW2cxOz-eO6"
      },
      "source": [
        "#### 2.1.b Visualizing and Comparing Topological and Geometric Measures (3 pts)\n",
        "\n",
        "Implement the following functions to visualize and compare topological properties of two given graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_node_degree(A):\n",
        "    \"\"\"Returns the degree (row sum) for each node.\"\"\"\n",
        "    return A.sum(dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# TODO: Implement the plotting functions\n",
        "# ==========================================\n",
        "\n",
        "def plot_node_degree_distribution_two_graphs(A1, A2, label1=\"Graph 1\", label2=\"Graph 2\"):\n",
        "    \"\"\"\n",
        "    Plots the node degree distributions of two graphs in a single figure.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def plot_betweenness_distribution_two_graphs(A1, A2, label1=\"Graph 1\", label2=\"Graph 2\"):\n",
        "    \"\"\"\n",
        "    Plots the betweenness centrality distributions of two graphs in a single figure.\n",
        "    Hint: Convert adjacency matrix to NetworkX graph for calculation.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def plot_ricci_curvature_distribution_two_graphs(A1, A2, label1=\"Graph 1\", label2=\"Graph 2\"):\n",
        "    \"\"\"\n",
        "    Plots the Ollivier-Ricci curvature distributions for two graphs.\n",
        "    Hint: Use GraphRicciCurvature to compute values, then plot densities.\n",
        "    Hint: Convert adjacency matrix to NetworkX graph to use GraphRicciCurvature (see the example in graph_ricci_curvature.py).\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-7C2Cir-qkx"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "G1_train = read_json_data(G1_TRAIN_PATH, has_label=True) \n",
        "G2_train = read_json_data(G2_TRAIN_PATH, has_label=True)\n",
        "\n",
        "# Extract Adjacency Matrices for the first graph in each set\n",
        "A1 = G1_train[0][1]\n",
        "A2 = G2_train[0][1]\n",
        "\n",
        "# Generate Plots\n",
        "plot_node_degree_distribution_two_graphs(A1, A2)\n",
        "plot_betweenness_distribution_two_graphs(A1, A2)\n",
        "plot_ricci_curvature_distribution_two_graphs(A1, A2) # might take a bit longer to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Discussion]** Compare the topologies of two graphs using the topological and geometrical measures that you computed. What are your observations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az5POuWw-xbI"
      },
      "source": [
        "#### 2.1.c Visualizing the Graphs (2 pts)\n",
        "\n",
        "Implement the function below to generate visual representations of both graphs. Comment on the topologies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsLllaMM-viv"
      },
      "outputs": [],
      "source": [
        "def plot_graph(A, y=None, title=\"Graph Visualization\"):\n",
        "    \"\"\"\n",
        "    Plots a graph defined by adjacency matrix A. \n",
        "    Nodes should be colored by labels y if provided.\n",
        "    \"\"\"\n",
        "    # ==========================================\n",
        "    # TODO: Implement graph visualization\n",
        "    # ==========================================\n",
        "    pass\n",
        "\n",
        "# Visualize\n",
        "plot_graph(G1_train[0][1], G1_train[0][2], title=\"Graph 1\")\n",
        "plot_graph(G2_train[0][1], G2_train[0][2], title=\"Graph 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Discussion]** Compare the topologies of two graphs. What are your observations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V019daiQ-40W"
      },
      "source": [
        "#### 2.1.d Visualizing Node Feature Distributions (2 pts)\n",
        "\n",
        "- Implement the function `plot_node_feature_dist_by_class_two_graphs` to visualize the average node feature distribution per class for two given graphs.\n",
        "\n",
        "- Do not consider the distribution of a specific feature $ x_i $, consider the mean of the feature vector $ \\mathbf{x} $ for each node.\n",
        "\n",
        "- The function should generate a plot similar to Figure 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXDuE-kU-var"
      },
      "outputs": [],
      "source": [
        "def plot_node_feature_dist_by_class_two_graphs(G1, G2):\n",
        "    \"\"\"\n",
        "    Plots the density of average node features by class for two graphs.\n",
        "    G1, G2: Tuples of (X, A, y)\n",
        "    \"\"\"\n",
        "    # ==========================================\n",
        "    # TODO: Implement feature distribution plot\n",
        "    # ==========================================\n",
        "    pass\n",
        "\n",
        "plot_node_feature_dist_by_class_two_graphs(G1_train[0], G2_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Discussion]** Analyze the results. Discuss any observed overlaps between the node feature distributions across different classes. Are the features well-separated, or do they exhibit significant overlap? Compare the distributions between the two graphs and provide insights into their differences and similariteis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwaVCQmz_ATi"
      },
      "source": [
        "### 2.2 -  Evaluating GCN Performance on Different Graph Structures (10 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmpJQpdP-vVW"
      },
      "outputs": [],
      "source": [
        "# --- Helper Functions for GCN ---\n",
        "\n",
        "def symmetric_normalize(A):\n",
        "    \"\"\"Performs symmetric normalization: D^{-1/2} * A * D^{-1/2}.\"\"\"\n",
        "    A_tilde = A + torch.eye(A.size(0)).to(A.device) # Add self-loops\n",
        "    d = A_tilde.sum(dim=1)\n",
        "    D_inv_sqrt = torch.diag(torch.pow(d, -0.5))\n",
        "    return D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    \"\"\"Pre-normalizes Adjacency matrices in a dataset list.\"\"\"\n",
        "    return [(X, symmetric_normalize(A), y) for X, A, y in dataset]\n",
        "\n",
        "def plot_training_and_validation(losses, val_f1s, title=\"Training Progress\"):\n",
        "    \"\"\"Plots training loss and validation F1 score.\"\"\"\n",
        "    epochs = range(1, len(losses) + 1)\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss', color='tab:red')\n",
        "    ax1.plot(epochs, losses, color='tab:red', label='Train Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.set_ylabel('F1 Score', color='tab:blue')\n",
        "    ax2.plot(epochs, val_f1s, color='tab:blue', label='Val F1')\n",
        "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkjN-SEa_GXo"
      },
      "outputs": [],
      "source": [
        "# --- Provided Base GCN Components ---\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.use_nonlinearity = use_nonlinearity\n",
        "        self.Omega = nn.Parameter(torch.randn(input_dim, output_dim) * (2.0 / (input_dim + output_dim))**0.5)\n",
        "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "    def forward(self, H, A_norm):\n",
        "        H_agg = torch.matmul(A_norm, H)\n",
        "        out = torch.matmul(H_agg, self.Omega) + self.beta\n",
        "        return F.relu(out) if self.use_nonlinearity else out\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, G_train, G_eval, num_epochs=100, lr=1e-3, verbose=True, return_embeddings=False):\n",
        "    X_tr, A_tr, y_tr = G_train\n",
        "    X_val, A_val, y_val = G_eval\n",
        "    \n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_hist, train_f1_hist, val_f1_hist = [], [], []\n",
        "    last_embeddings = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(A_tr, X_tr, return_embeddings=return_embeddings)\n",
        "        pred, embs = output if return_embeddings else (output, None)\n",
        "        \n",
        "        loss = F.binary_cross_entropy(pred.squeeze(), y_tr.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_hist.append(loss.item())\n",
        "        \n",
        "        # Evaluation\n",
        "        train_f1 = evaluate_model(model, X_tr, A_tr, y_tr)\n",
        "        val_f1 = evaluate_model(model, X_val, A_val, y_val)\n",
        "        \n",
        "        train_f1_hist.append(train_f1)\n",
        "        val_f1_hist.append(val_f1)\n",
        "        \n",
        "        if return_embeddings: last_embeddings = embs\n",
        "\n",
        "        if verbose and (epoch+1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | Train F1: {train_f1:.3f} | Val F1: {val_f1:.3f}\")\n",
        "\n",
        "    return (loss_hist, train_f1_hist, val_f1_hist, last_embeddings) if return_embeddings else (loss_hist, train_f1_hist, val_f1_hist)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, X, A, y):\n",
        "    model.eval()\n",
        "    pred = model(A, X)\n",
        "    if isinstance(pred, tuple): pred = pred[0] # Handle return_embeddings case\n",
        "    \n",
        "    y_pred = (pred.squeeze() >= 0.5).long().cpu().numpy()\n",
        "    y_true = y.cpu().numpy()\n",
        "    return precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvu2VEul_V31"
      },
      "source": [
        "#### 2.2.a - Implementation of Layered GCN (1 pt)\n",
        "\n",
        "* Modify the `GraphNeuralNetwork` class so that `num_layers` can be passed as an argument.\n",
        "* Ensure it returns all embedding layers if requested (for t-SNE analysis).\n",
        "* Train on G1 and G2 independently and plot results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzB_namE_GQ6"
      },
      "outputs": [],
      "source": [
        "class GraphNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
        "        super().__init__()\n",
        "        # ==========================================\n",
        "        # TODO: Define layers dynamically based on num_layers\n",
        "        # ==========================================\n",
        "        pass\n",
        "\n",
        "    def forward(self, A, X, return_embeddings=False):\n",
        "        # ==========================================\n",
        "        # TODO: Implement forward pass storing intermediate embeddings\n",
        "        # ==========================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0435Sb2P_GOh"
      },
      "outputs": [],
      "source": [
        "# Prepare Data\n",
        "G1_tr_p = prepare_dataset(G1_train)[0]\n",
        "G1_ev_p = prepare_dataset(read_json_data(G1_EVAL_PATH))[0]\n",
        "G2_tr_p = prepare_dataset(G2_train)[0]\n",
        "G2_ev_p = prepare_dataset(read_json_data(G2_EVAL_PATH))[0]\n",
        "\n",
        "MODEL_PARAMS = {\"input_dim\": 10, \"hidden_dim\": 8, \"num_layers\": 4}\n",
        "TRAIN_PARAMS = {\"num_epochs\": 200, \"lr\": 0.0005, \"verbose\": True}\n",
        "\n",
        "print(\"--- Training on Graph 1 ---\")\n",
        "set_seed(42)\n",
        "model_g1 = GraphNeuralNetwork(**MODEL_PARAMS).to(device)\n",
        "loss1, _, f1_1 = train_model(model_g1, G1_tr_p, G1_ev_p, **TRAIN_PARAMS)\n",
        "plot_training_and_validation(loss1, f1_1, title=\"Graph 1 Training\")\n",
        "\n",
        "print(\"\\n--- Training on Graph 2 ---\")\n",
        "set_seed(42)\n",
        "model_g2 = GraphNeuralNetwork(**MODEL_PARAMS).to(device)\n",
        "loss2, _, f1_2 = train_model(model_g2, G2_tr_p, G2_ev_p, **TRAIN_PARAMS)\n",
        "plot_training_and_validation(loss2, f1_2, title=\"Graph 2 Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DI06VmU_jQP"
      },
      "source": [
        "#### 2.2.b - Plotting t-SNE Embeddings (1 pt)\n",
        "\n",
        "Use t-SNE to visualize the node embeddings from the **final hidden layer** of the GCN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-mPb1nc_GLb"
      },
      "outputs": [],
      "source": [
        "def plot_tsne(embeddings, labels, title=\"t-SNE\"):\n",
        "    \"\"\"\n",
        "    Plots t-SNE visualization of embeddings.\n",
        "    embeddings: Tensor of shape (N, D)\n",
        "    labels: Tensor of shape (N,)\n",
        "    \"\"\"\n",
        "    # ==========================================\n",
        "    # TODO: Implement t-SNE plotting, you may use TSNE from sklearn (check imports)\n",
        "    # ==========================================\n",
        "    pass\n",
        "\n",
        "# Retrain to capture embeddings\n",
        "print(\"Generating Embeddings...\")\n",
        "set_seed(42)\n",
        "_, _, _, embs1 = train_model(model_g1, G1_tr_p, G1_ev_p, num_epochs=1, return_embeddings=True, verbose=False)\n",
        "plot_tsne(embs1[-2], G1_tr_p[2], title=\"Graph 1 Embeddings\")\n",
        "\n",
        "set_seed(42)\n",
        "_, _, _, embs2 = train_model(model_g2, G2_tr_p, G2_ev_p, num_epochs=1, return_embeddings=True, verbose=False)\n",
        "plot_tsne(embs2[-2], G2_tr_p[2], title=\"Graph 2 Embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP-MI6Px_tiW"
      },
      "source": [
        "#### 2.2.c - Training on Merged Graphs (2 pts)\n",
        "\n",
        "* Complete `unify_two_graphs` to create $G_{union} = G_1 \\cup G_2$.\n",
        "* Train the GCN on this unified graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unify_two_graphs(G1, G2):\n",
        "    \"\"\"\n",
        "    Merges two graphs (disjoint union).\n",
        "    G1, G2: Tuples of (X, A, y)\n",
        "    Returns: (X_union, A_union, y_union)\n",
        "    \"\"\"\n",
        "    # ==========================================\n",
        "    # TODO: Implement graph union\n",
        "    # ==========================================\n",
        "    pass\n",
        "\n",
        "# Create Union Dataset\n",
        "G_union_tr = unify_two_graphs(G1_tr_p, G2_tr_p) # Already normalized individually, but check math if re-norm needed\n",
        "G_union_ev = unify_two_graphs(G1_ev_p, G2_ev_p)\n",
        "\n",
        "# Train on Union\n",
        "print(\"\\n--- Training on Unified Graph ---\")\n",
        "set_seed(42)\n",
        "model_union = GraphNeuralNetwork(**MODEL_PARAMS).to(device)\n",
        "loss_u, _, f1_u, embs_u = train_model(model_union, G_union_tr, G_union_ev, **TRAIN_PARAMS, return_embeddings=True)\n",
        "\n",
        "plot_training_and_validation(loss_u, f1_u, title=\"Unified Graph Training\")\n",
        "plot_tsne(embs_u[-2], G_union_tr[2], title=\"Unified Graph Embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxO0zPK3_4Rz"
      },
      "source": [
        "#### 2.2.d Merged vs. Independent Training (6 pts)\n",
        "\n",
        "In this part, analyze and compare the training on two graphs independently\n",
        "($G_1$, $G_2$) versus training on their union $G_{\\text{union}} = G_1 \\cup G_2$.\n",
        "\n",
        "Your analysis must include **both empirical evidence and conceptual reasoning**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Required analysis\n",
        "\n",
        "1. **Training behaviour**\n",
        "   - Compare convergence trends\n",
        "   - Comment on stability, speed of convergence, and any signs of underfitting or overfitting.\n",
        "\n",
        "2. **Learned representations**\n",
        "   - Compare node embeddings obtained from:\n",
        "     - independent training on $G_1$ and $G_2$, and\n",
        "     - merged training on $G_{\\text{union}}$.\n",
        "\n",
        "3. **Overall performance comparison**\n",
        "   - Summarise differences in predictive performance and representation quality.\n",
        "   - Clearly state **what changes** when moving from independent to merged training.\n",
        "\n",
        "---\n",
        "\n",
        "#### Hypothesis formation\n",
        "\n",
        "- Formulate a clear hypothesis explaining **why** the observed differences occur.\n",
        "- Your hypothesis should explicitly reference:\n",
        "  - graph structure,\n",
        "  - information mixing across graphs,\n",
        "  - and the inductive bias of the model you are using.\n",
        "\n",
        "---\n",
        "\n",
        "#### Trade-offs and design discussion\n",
        "\n",
        "Discuss the **advantages and drawbacks** of merged vs. independent training, considering:\n",
        "\n",
        "- **Computational aspects**  \n",
        "  (training time, memory usage, scalability)\n",
        "\n",
        "- **Modeling aspects**  \n",
        "  (representation entanglement, negative transfer, over-smoothing, or regularisation effects)\n",
        "\n",
        "- **Practical considerations**  \n",
        "  (when merged training might be desirable or risky in real-world settings)\n",
        "\n",
        "Then propose **concrete model or optimisation or data modifications** that could mitigate the drawbacks\n",
        "(e.g. architectural changes, loss terms, sampling strategies, training protocols or data modifications such as augmentations etc.).\n",
        "\n",
        "You do **NOT** need to implement your proposal to get full points from this question. It is optional (see the bonus below).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Bonus (optional)**\n",
        "\n",
        "Implement your proposal for learning on  \n",
        "$G_{\\text{union}} = G_1 \\cup G_2$ in a better way \n",
        "and compare its outcomes against both independent and naïve merged training.\n",
        "\n",
        "Clearly explain:\n",
        "- what you changed,\n",
        "- why it should help,\n",
        "- and whether the results support your expectation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFG1hnSG_-Of"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# BONUS (optional)\n",
        "# ####################################################\n",
        "# MODIFY THE CODE BELOW\n",
        "# ####################################################\n",
        "\n",
        "\n",
        "# This part is optional. Feel free to implement a new GNN architecture, a new training procedure etc.\n",
        "# You can call the provided helper functions (feel free to write your own functions for plotting)\n",
        "# plot_training_and_validation\n",
        "# plot_tsne\n",
        "\n",
        "# ####################################################\n",
        "# END OF MODIFICATION\n",
        "# ####################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oafH-mgIAFab"
      },
      "source": [
        "### 2.3 Topological Changes to Improve Training (10 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH5cXlYBAHuL"
      },
      "source": [
        "#### 2.3.a - Plot the Ricci Curvature for each edge (1 pt)\n",
        "\n",
        "- Create a bar plot showing the Ricci curvature for each edge. The x-axis should represent the edges, and the y-axis should represent their corresponding Ricci curvature values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiEiKjXWALEu"
      },
      "outputs": [],
      "source": [
        "def plot_ricci_per_edge(A):\n",
        "    \"\"\"\n",
        "    Plots the Ollivier-Ricci curvature for each edge\n",
        "    defined by adjacency matrix A (PyTorch tensor).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    A : torch.Tensor\n",
        "        Adjacency matrix for an undirected graph (NxN).\n",
        "    \n",
        "    Follow the example in graph_ricci_curvature python script.\n",
        "\n",
        "    You can use nx.from_numpy_array to convert the adjacency matrix to a NetworkX graph.\n",
        "    \n",
        "    \"\"\"\n",
        "\t# ==========================================\n",
        "    # TODO: Implement Ricci edge plot\n",
        "    # ==========================================\n",
        "    pass\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "plot_ricci_per_edge(G1_train[0][1]) # might take some time to run\n",
        "plot_ricci_per_edge(G2_train[0][1]) # might take some time to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAobPzD5ANzA"
      },
      "source": [
        "#### 2.3.b Extreme Topological Scenarios (4 points)\n",
        "\n",
        "In this question, you will explore limit cases of graph topology to understand what role the graph\n",
        "structure plays in GNN learning. Please do the following tasks for both graphs (G1 and G2). If your reasoning and technique is the same for both graphs, you do not need to explain the same thing twice. Still, please run your implementation on both graphs. \n",
        "\n",
        "1. **Topology removal**  \n",
        "   Construct a version of the graph where neighborhood aggregation becomes meaningless, such that\n",
        "   the GNN behaves similarly to a node-feature-only MLP.\n",
        "   - Describe precisely how you modified the graph structure.\n",
        "   - Train the GCN on this modified graph and report the performance.\n",
        "\n",
        "2. **Oracle topology**  \n",
        "   Construct an idealized graph assuming label information is available **only for analysis purposes**\n",
        "   (not as input features).\n",
        "   - This means you are allowed to look at the labels to design or modify the graph, but you are not allowed to give the labels to the model as input features. The model still receives only node features and graph structure and is trained normally using labels only in the loss function. \n",
        "   - Describe what edges you add or remove and why this topology should be easy for a GNN.\n",
        "   - Train the GCN on this graph and report the performance.\n",
        "\n",
        "3. **Analysis:**  \n",
        "   Compare the results from:\n",
        "   - the original graph,\n",
        "   - the topology-removed graph,\n",
        "   - the oracle graph.\n",
        "   \n",
        "   Discuss:\n",
        "   - What these extreme cases reveal about the relationship between graph topology and node labels.\n",
        "   - Why the original GCN behaves the way it does on this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Topology removal**\n",
        "\n",
        "Explain your reasoning and methodology\n",
        "\n",
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def topology_removal(G):\n",
        "    \"\"\"Removes informative topology.\"\"\"\n",
        "    X, A, y = G\n",
        "    # ==========================================\n",
        "    # TODO: Modify A\n",
        "    # ==========================================\n",
        "    return X, A, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Oracle topology**\n",
        "\n",
        "Explain your reasoning and methodology\n",
        "\n",
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def oracle_topology(G):\n",
        "    \"\"\"Constructs ideal topology. You may use the labels y to modify A.\"\"\"\n",
        "    X, A, y = G\n",
        "    # ==========================================\n",
        "    # TODO: Modify A\n",
        "    # ==========================================\n",
        "    return X, A, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwGk_tdzASjq"
      },
      "outputs": [],
      "source": [
        "def run_topology_experiment(strategy_fn, G_tr, G_ev, title):\n",
        "    print(f\"\\n--- Experiment: {title} ---\")\n",
        "    \n",
        "    # Apply strategy and re-normalize\n",
        "    G_tr_mod = prepare_dataset([strategy_fn(G_tr)])[0]\n",
        "    G_ev_mod = prepare_dataset([strategy_fn(G_ev)])[0]\n",
        "    \n",
        "    set_seed(42)\n",
        "    model = GraphNeuralNetwork(**MODEL_PARAMS).to(device)\n",
        "    loss, _, f1 = train_model(model, G_tr_mod, G_ev_mod, **TRAIN_PARAMS)\n",
        "    print(f\"Final Val F1: {f1[-1]:.4f}\")\n",
        "    plot_training_and_validation(loss, f1, title=title)\n",
        "\n",
        "# Run Experiments\n",
        "# for both G1 and G2\n",
        "set_seed(42)\n",
        "\n",
        "run_topology_experiment(topology_removal, G1_train[0], read_json_data(G1_EVAL_PATH)[0], \"Topology Removal (G1)\")\n",
        "run_topology_experiment(oracle_topology, G1_train[0], read_json_data(G1_EVAL_PATH)[0], \"Oracle Topology (G1)\")\n",
        "\n",
        "run_topology_experiment(topology_removal, G2_train[0], read_json_data(G2_EVAL_PATH)[0], \"Topology Removal (G2)\")\n",
        "run_topology_experiment(oracle_topology, G2_train[0], read_json_data(G2_EVAL_PATH)[0], \"Oracle Topology (G2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfbS5BKAXSZ"
      },
      "source": [
        "#### 2.3.c Graph Augmentation Without Labels (5 pts)\n",
        "\n",
        "In Question 2.3.b, you constructed an **Oracle graph** using labels in order to estimate the\n",
        "*theoretical upper bound* on performance.  \n",
        "That experiment was **diagnostic only**.\n",
        "\n",
        "In this question, your goal is to design a **practical graph augmentation strategy**\n",
        "that could be applied **without access to labels**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Task\n",
        "\n",
        "Implement a graph augmentation method that modifies the adjacency matrix $A$\n",
        "using **only**:\n",
        "- node features $X$, and/or\n",
        "- the original graph topology $A$ itself.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Important considerations\n",
        "\n",
        "- **No labels**  \n",
        "  You are **strictly forbidden** from using the labels $y$ (directly or indirectly)\n",
        "  to decide on modification of graph.\n",
        "\n",
        "- **Generalisability**  \n",
        "  Your method should ideally apply to **both graphs** $G_1$ and $G_2$.\n",
        "  If your approach only works well for one graph, you may implement\n",
        "  two separate versions of `augment_graph`.\n",
        "  In that case, clearly explain:\n",
        "  - why the method does not generalise,\n",
        "  - what structural differences between the graphs cause this limitation.\n",
        "\n",
        "- **Performance is not the only objective**  \n",
        "  It is expected that your augmentation improves classification performance\n",
        "  relative to training on the original $G_1$ and $G_2$.\n",
        "  If performance does not improve, you may instead argue that your method:\n",
        "  - reduces computational cost,\n",
        "  - improves training stability,\n",
        "  - simplifies the graph structure,\n",
        "  - etc.\n",
        "\n",
        "  In all cases, you must clearly explain **what improves (or fails to improve) and why**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Evaluation and analysis\n",
        "\n",
        "- Compare results obtained from:\n",
        "  - the original graphs,\n",
        "  - your augmented graphs,\n",
        "- Use consistent models, training settings, hyperparameters to be fair.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNyrYT6sAaq_"
      },
      "outputs": [],
      "source": [
        "def augment_graph(G):\n",
        "    \"\"\"\n",
        "    Augments the graph structure using only features (X) and existing topology (A).\n",
        "    \n",
        "    Args:\n",
        "        G: Tuple (X, A, y)\n",
        "    \n",
        "    Returns:\n",
        "        X, A_new, y\n",
        "        \n",
        "    IMPORTANT: You must NOT use 'y' (labels) to construct A_new. \n",
        "    \"\"\"\n",
        "    X, A, y = G\n",
        "    \n",
        "    # ==========================================\n",
        "    # TODO: Implement your augmentation strategy\n",
        "    # ==========================================\n",
        "    \n",
        "    return X, A, y\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(\"\\n--- Realistic Augmentation Training ---\")\n",
        "# We use the same runner as before, but this time using the \"augment_graph\" function\n",
        "run_topology_experiment(augment_graph, G1_train[0], read_json_data(G1_EVAL_PATH)[0], \"Realistic Augmentation (G1)\")\n",
        "\n",
        "run_topology_experiment(augment_graph, G2_train[0], read_json_data(G2_EVAL_PATH)[0], \"Realistic Augmentation (G2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpretation\n",
        "\n",
        "Explain:\n",
        "- your methodology,\n",
        "- what change you introduced to the graph,\n",
        "- what assumption this change encodes about the data,\n",
        "- and why this assumption helps or fails on this dataset.\n",
        "\n",
        "Support your discussion with **quantitative results and plots** where appropriate.\n",
        "\n",
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thdlVEyeHJkp"
      },
      "source": [
        "## Question 3 - Mystery Graph Investigation (30 pts)\n",
        "\n",
        "In this question, you will work with a **mystery graph dataset** and investigate the behavior of graph-based learning.\n",
        "\n",
        "You will proceed in three stages:\n",
        "\n",
        "1. **Train baseline models** and observe their performance.\n",
        "2. **Diagnose and explain** unexpected behavior using analysis and visualisation.\n",
        "3. **Design and implement your own method** to improve performance.\n",
        "\n",
        "You are not told anything about the structure of the dataset in advance.\n",
        "Your task is to *discover* what is happening and respond accordingly.\n",
        "\n",
        "### Rules and Constraints\n",
        "\n",
        "- You may only use **NumPy** and **PyTorch**.\n",
        "- Do not modify the dataset.\n",
        "- You may define additional helper functions if needed.\n",
        "- Written answers must be provided in the Markdown cells.\n",
        "\n",
        "This question rewards **reasoning and diagnosis**, not just performance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6NUr0S4HILW"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5WXzQASHepD"
      },
      "outputs": [],
      "source": [
        "with open(\"data/mystery_graph.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "x = torch.tensor(data[\"x\"], dtype=torch.float32, device=device)\n",
        "y = torch.tensor(data[\"y\"], dtype=torch.long, device=device)\n",
        "\n",
        "src = torch.tensor(data[\"src\"], dtype=torch.long, device=device)\n",
        "dst = torch.tensor(data[\"dst\"], dtype=torch.long, device=device)\n",
        "edge_w = torch.tensor(data[\"edge_w\"], dtype=torch.float32, device=device)\n",
        "\n",
        "train_mask = torch.tensor(data[\"train_mask\"], dtype=torch.bool, device=device)\n",
        "val_mask   = torch.tensor(data[\"val_mask\"], dtype=torch.bool, device=device)\n",
        "test_mask  = torch.tensor(data[\"test_mask\"], dtype=torch.bool, device=device)\n",
        "\n",
        "N, D = x.shape\n",
        "C = int(data[\"num_classes\"])\n",
        "\n",
        "print(f\"N={N}, D={D}, C={C}\")\n",
        "print(f\"Directed edges: {src.numel()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyBCCkE1JIxw"
      },
      "source": [
        "### 3.1 Baseline Models\n",
        "\n",
        "You will first train two baseline models:\n",
        "\n",
        "- **MLP**: ignores the graph completely.\n",
        "- **GCN**: uses mean aggregation over neighbors.\n",
        "\n",
        "At this stage, **do not try to improve anything**.\n",
        "Simply train both models and observe their performance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqhDT_DsJMC3"
      },
      "outputs": [],
      "source": [
        "def macro_f1(pred, target, num_classes):\n",
        "    f1s = []\n",
        "    for c in range(num_classes):\n",
        "        tp = ((pred == c) & (target == c)).sum().item()\n",
        "        fp = ((pred == c) & (target != c)).sum().item()\n",
        "        fn = ((pred != c) & (target == c)).sum().item()\n",
        "        prec = tp / (tp + fp + 1e-12)\n",
        "        rec  = tp / (tp + fn + 1e-12)\n",
        "        f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec + 1e-12)\n",
        "        f1s.append(f1)\n",
        "    return float(sum(f1s) / len(f1s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5lUebWIXuC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mean_aggregate(h, src, dst, num_nodes):\n",
        "    out = torch.zeros((num_nodes, h.size(1)), device=h.device)\n",
        "    out.index_add_(0, dst, h[src])\n",
        "    deg = torch.zeros((num_nodes,), device=h.device)\n",
        "    deg.index_add_(0, dst, torch.ones_like(dst, dtype=h.dtype))\n",
        "    return out / (deg.unsqueeze(-1) + 1e-12)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, n_classes, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in, d_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_hidden, n_classes),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, n_classes, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_in, d_hidden)\n",
        "        self.lin2 = nn.Linear(d_hidden, n_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, src, dst, num_nodes):\n",
        "        h = mean_aggregate(x, src, dst, num_nodes)\n",
        "        h = F.relu(self.lin1(h))\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = mean_aggregate(h, src, dst, num_nodes)\n",
        "        return self.lin2(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf7Daf7IJSYN"
      },
      "outputs": [],
      "source": [
        "def train_model(model, forward_fn, epochs=80, lr=1e-3):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val = -1.0\n",
        "    best_state = None\n",
        "    history = {\"train_f1\": [], \"val_f1\": []}\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        logits = forward_fn()\n",
        "        loss = F.cross_entropy(logits[train_mask], y[train_mask])\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = forward_fn().argmax(dim=-1)\n",
        "            tr = macro_f1(pred[train_mask], y[train_mask], C)\n",
        "            va = macro_f1(pred[val_mask], y[val_mask], C)\n",
        "\n",
        "        history[\"train_f1\"].append(tr)\n",
        "        history[\"val_f1\"].append(va)\n",
        "\n",
        "        if va > best_val:\n",
        "            best_val = va\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        if ep == 1 or ep % 10 == 0:\n",
        "            print(f\"Epoch {ep:03d} | loss={loss.item():.4f} | trainF1={tr:.3f} | valF1={va:.3f}\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state, strict=True)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, forward_fn):\n",
        "    model.eval()\n",
        "    logits = forward_fn()\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    return (\n",
        "        macro_f1(pred[train_mask], y[train_mask], C),\n",
        "        macro_f1(pred[val_mask], y[val_mask], C),\n",
        "        macro_f1(pred[test_mask], y[test_mask], C),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9OW2pMHJOOB"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "mlp = MLP(D, 64, C).to(device)\n",
        "hist_mlp = train_model(mlp, lambda: mlp(x))\n",
        "print(\"MLP  train/val/test:\", eval_model(mlp, lambda: mlp(x)))\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "gcn = GCN(D, 64, C).to(device)\n",
        "hist_gcn = train_model(gcn, lambda: gcn(x, src, dst, N))\n",
        "print(\"GCN  train/val/test:\", eval_model(gcn, lambda: gcn(x, src, dst, N)))\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(hist_mlp[\"val_f1\"], label=\"MLP\")\n",
        "plt.plot(hist_gcn[\"val_f1\"], label=\"GCN\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Macro-F1\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKGinSiqJZZs"
      },
      "source": [
        "### 3.2 Explaining the Results (15 pts)\n",
        "\n",
        "You should observe that **the MLP performs better than the GCN**.\n",
        "\n",
        "This is not what one might expect if the graph were always helpful.\n",
        "\n",
        "#### Task\n",
        "Using **quantitative measures and visualisations**, explain:\n",
        "\n",
        "1. Why does the GCN perform worse than a feature-only MLP? Considering a GCN does neighborhood aggregation, it is not expected an MLP to perform better than a GCN. Explain this phenomenon. \n",
        "2. What is the relationship between node labels and the graph structure? \n",
        "3. What kind of information does **mean aggregation** preserve or destroy here?\n",
        "4. How is this phenomenon related to homophily in graphs and GNNs?\n",
        "\n",
        "You can reuse or extend analyses from earlier or your are encouraged to introduce new diagnostics if needed.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00EW-yzIJVaF"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Add diagnostics to support your explanation.\n",
        "# Make an analysis on graph and model properties to explain the results you obtained.\n",
        "# What properties of the graph and the model do you think contributed to the results you obtained? \n",
        "# Write your code that supports your explainations to these questions.\n",
        "\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sp5BoXPJkr9"
      },
      "source": [
        "#### 3.2 Answer\n",
        "\n",
        "**Why does the GCN perform worse than a feature-only MLP? Considering a GCN does neighborhood aggregation, it is not expected an MLP to perform better than a GCN. Explain this phenomenon.**  \n",
        "...\n",
        "\n",
        "**What is the relationship between node labels and the graph structure?**  \n",
        "...\n",
        "\n",
        "**What kind of information does \\textbf{mean aggregation} preserve or destroy here?**  \n",
        "...\n",
        "\n",
        "**How is this phenomenon related to homophily in graphs and GNNs?**\n",
        "...\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQl4pj7-Jo33"
      },
      "source": [
        "### 3.3 Designing a Better Graph Method (15 pts)\n",
        "\n",
        "The GCN fails because of *how* it aggregates information from neighbors.\n",
        "\n",
        "Your task is to design a **new message passing method** that:\n",
        "\n",
        "- uses the graph,\n",
        "- but avoids the failure mode of mean aggregation on this dataset.\n",
        "\n",
        "Your solution needs to\n",
        "\n",
        "- address the failure reason that you discovered in Stage 2,\n",
        "- be sophisticated rather than simply adding more layers and increasing number of learnable parameters.\n",
        "\n",
        "If your reasoning is about the optimization strategy (e.g., new optimizers instead of Adam) rather than the architecture itself, you need to apply your new optimization strategy to classical GCN and MLP too in order to be fair.\n",
        "\n",
        "There is **no single correct architecture**.\n",
        "What matters is that your design is **well-motivated** and **improves performance**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Zj7m-nfJnqK"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# Define any helper function(s) you need.\n",
        "# Feel free to modify the following class according to your needs, it is given just as a skeleton code.\n",
        "\n",
        "pass\n",
        "\n",
        "\n",
        "class MyGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Your custom graph model.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_in, d_hidden, n_classes):\n",
        "        super().__init__()\n",
        "        # TODO: define layers\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, src, dst, edge_w, num_nodes):\n",
        "        # TODO: implement forward pass\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycP_cSnCKucy"
      },
      "outputs": [],
      "source": [
        "set_seed(42) # use the same seed for reproducibility\n",
        "\n",
        "# TODO:\n",
        "# 1) Instantiate your model\n",
        "# 2) Train it using train_model\n",
        "# 3) Report train/val/test F1\n",
        "# 4) Plot validation curve alongside MLP and GCN\n",
        "\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6-T7Wt8Kz1C"
      },
      "source": [
        "#### 3.3 Final Explanation\n",
        "\n",
        "Describe:\n",
        "\n",
        "1. What failure mode you identified in the GCN.\n",
        "2. How your method addresses this failure.\n",
        "3. Why your method improves performance on this dataset.\n",
        "4. What assumptions your method makes about the graph.\n",
        "\n",
        "Your answer should clearly connect:\n",
        "- dataset structure,\n",
        "- aggregation behavior,\n",
        "- and model design.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[Answer]**\n",
        "\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
